#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass beamer
\begin_preamble
\usetheme{metropolis}
\end_preamble
\options aspectratio=169
\use_default_options true
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "helvet" "default"
\font_typewriter "courier" "default"
\font_math "newtxmath" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style british
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Module 5: Advanced inference methods
\end_layout

\begin_layout Section
Variational inference
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Overview
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
So far we have discussed MAP and MCMC methods for approximating posteriors.
\end_layout

\begin_layout Itemize
These strategies represent extremes in terms of accuracy and computational
 complexity.
\end_layout

\begin_layout Itemize
There are many other approaches which lie between MAP and MCMC with different
 trade-offs between computational complexity and accuracy.
\end_layout

\begin_layout Itemize
Variational inference (VI) is one such approach.
\end_layout

\begin_deeper
\begin_layout Itemize
In practice VI is typically not much more computationally expensive than
 MAP, but provides much better posterior approximations.
\end_layout

\end_deeper
\begin_layout Itemize
There has been a renewed interest in VI over the last five years.
\end_layout

\begin_deeper
\begin_layout Itemize
As a result VI is much easier to use on a wide variety of problems.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Basics
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The main idea is to frame posterior approximation as an optimization function.
\end_layout

\begin_layout Itemize
To objective of the optimisation function is some measure of similarity
 between the posterior, 
\begin_inset Formula $p(\theta|X)$
\end_inset

, and the variational approximation 
\begin_inset Formula $q(\theta|\lambda)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
The parameter 
\begin_inset Formula $\lambda$
\end_inset

 is the variational parameters which are optimised to find the best solution.
\end_layout

\end_deeper
\begin_layout Itemize
The typical measure of similarity is the KL divergence.
\begin_inset Formula 
\begin{eqnarray*}
KL(q,p) & = & \int\log\frac{p(\theta|X)}{q(\theta|\lambda)}q(\theta|\lambda){\rm d}\theta\\
 & = & \mathbb{E}_{q}[\log p(\theta,X)]-\mathbb{E}_{q}[\log q(\theta|\lambda)]+\log p(X)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
Since 
\begin_inset Formula $p(X)$
\end_inset

 is a constant with respect to 
\begin_inset Formula $\lambda$
\end_inset

 optimising the KL is equivalent to optimising the evidence lower bound
 (ELBO)
\begin_inset Formula 
\begin{eqnarray*}
\mathcal{L} & := & \mathbb{E}_{q}[\log p(\theta,X)]-\mathbb{E}_{q}[\log q(\theta|\lambda)]
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Mean field variational inference
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Historically, mean field variational inference (MFVI) often called variational
 Bayes was the most common approach.
\end_layout

\begin_layout Itemize
MFVI assumes the approximating distribution has a factorised form
\begin_inset Formula 
\begin{eqnarray*}
q(\theta|\lambda) & = & \prod_{i}q_{i}(\theta_{i}|\lambda_{i})
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
The factorisation is part of the approximation, and has nothing to do with
 the model structure.
\end_layout

\begin_layout Itemize
If the model is in the conjugate exponential family then it is possible
 to obtain an algorithm similar to EM with closed form updates.
\end_layout

\begin_deeper
\begin_layout Itemize
In general this is not possible because 
\begin_inset Formula $\mathbb{E}_{q}[\log p(\theta,X)]$
\end_inset

 is hard to evaluate
\end_layout

\end_deeper
\begin_layout Itemize
The main weakness of MFVI is the approximate posterior cannot capture correlatio
n between parameters.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Monte Carlo and VI
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
To optimise the ELBO we need to be able to compute
\begin_inset Formula 
\begin{eqnarray*}
\nabla_{\lambda}\mathcal{L} & = & \nabla_{\lambda}[\mathbb{E}_{q}[\log p(\theta,X)]-\mathbb{E}_{q}[\log q(\theta|\lambda)]]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
One line of recent work has been figuring out how to make this computation
 feasible in general.
\end_layout

\begin_deeper
\begin_layout Itemize
One strategy is to reparameterise the variational distribution so the objective
 is differentiable.
\end_layout

\begin_layout Itemize
Another strategy is to move the gradient inside the expectation.
\end_layout

\end_deeper
\begin_layout Itemize
The main insight of the new approaches is that we can use Monte Carlo methods
 to approximate expectations.
\end_layout

\begin_layout Itemize
Applying these methods, a much broader class of variational distributions
 can used.
\end_layout

\begin_deeper
\begin_layout Itemize
This leads to better approximation of the posterior.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Stochastic VI
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The other major highlight of recent VI research has been the observation
 you can sub-sample data.
\end_layout

\begin_layout Itemize
The key idea is that we do not need to exactly evaluate the gradient of
 the ELBO.
\end_layout

\begin_layout Itemize
Instead we can use unbiased estimates to perform stochastic gradient descent.
\end_layout

\begin_layout Itemize
By using sub-sampling VI can efficiently scale to extremely large problems.
\end_layout

\end_deeper
\begin_layout Section
Advance Monte Carlo Methods
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Motivation
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
MH can struggle on high dimensional problems.
\end_layout

\begin_layout Itemize
The blocking procedure helps, but works poorly if the parameters are highly
 correlated between blocks.
\end_layout

\begin_layout Itemize
MH can also be slow to explore multi-modal posteriors or get trapped in
 local optima.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Simulated annealing
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Simulated annealing (SA) addresses the problem of local optima.
\end_layout

\begin_layout Itemize
The basic idea is to introduce a sequence of distributions such as
\begin_inset Formula 
\begin{eqnarray*}
p_{\beta}(\theta|X) & = & p(X|\theta)^{\beta}p(\theta)
\end{eqnarray*}

\end_inset

with 
\begin_inset Formula $0=\beta_{1}<\beta_{2}<\ldots<\beta_{T}=1$
\end_inset

.
\end_layout

\begin_layout Itemize
The hope is we start with a distribution that is easy to sample from, and
 the slowly change the distribution until we arrive at the one of interest.
\end_layout

\begin_layout Itemize
We start with 
\begin_inset Formula $\beta_{1}$
\end_inset

 and use any MCMC update for the parameters.
 After several iterations we then update to 
\begin_inset Formula $\beta_{2}$
\end_inset

 and so on.
\end_layout

\begin_layout Itemize
We collect samples when we reach 
\begin_inset Formula $\beta_{T}$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Hamiltonian Monte Carlo
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
HMC is an efficient way to update high dimensional continuous parameters.
\end_layout

\begin_layout Itemize
The basic idea is to propose a new value for the MCMC sampler by deterministical
ly moving along a path defined by Hamiltonian dynamics.
\end_layout

\begin_layout Itemize
Traditionally tuning the algorithm parameters has been hard.
 Recent work has provided automated tuning solutions that seem to work well.
\end_layout

\begin_layout Itemize
The probabilistic programming language STAN uses HMC and automatic differentiati
on to efficiently sample from a large class of user defined models.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
HMC in action
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename /home/andrew/Documents/teaching/CONTRA/figures/module_5/hmc.png
	lyxscale 10

\end_inset


\begin_inset Newline newline
\end_inset


\size tiny
Lan et al.
 Journal Journal of Computational and Graphical Statistics 2015
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Sequential Monte Carlo
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
SMC is a very general algorithm for sampling from high dimensional distributions.
\end_layout

\begin_layout Itemize
The basic idea is to break the sampling problem down into smaller and easier
 to sample problems.
\end_layout

\begin_layout Itemize
SMC uses a collection of particles to approximate the posterior.
\end_layout

\begin_layout Itemize
The particles are iteratively update using local moves and resampling.
\end_layout

\begin_layout Itemize
Traditionally SMC has been used for models with a natural sequential structure.
\end_layout

\begin_layout Itemize
Recent work has shown that it can be applied more generally for problems
 such as phylogenetics.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
SMC for phylogenetics
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename /home/andrew/Documents/teaching/CONTRA/figures/module_5/smc_phylo.png
	lyxscale 10
	height 85theight%

\end_inset


\begin_inset Newline newline
\end_inset


\size tiny
https://arxiv.org/abs/1806.08813
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Particle Gibbs
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The basic idea of PG is to use SMC to perform block updates of a subset
 of parameters in a larger MCMC framework.
\end_layout

\begin_layout Itemize
One example would be to use SMC update the hidden states of an HMM.
 MH or other moves could then be used to update the other parameters.
\end_layout

\begin_layout Itemize
The obvious way to implement this is incorrect.
 So PG relies on conditional SMC to produce a valid algorithm.
\end_layout

\begin_layout Itemize
Like SMC, PG was originally applied to models with sequential structure.
\end_layout

\begin_layout Itemize
Recent work has show it is more general and can be applied to problems such
 as Bayesian mixture models.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Particle Gibbs for mixture models
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename /home/andrew/Documents/teaching/CONTRA/figures/module_5/pgsm_swarm.pdf
	lyxscale 10
	height 85theight%

\end_inset


\begin_inset Newline newline
\end_inset


\size tiny
Bouchard-Côté et al.
 JMLR 2017
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Parallel tempering
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
PT is a general approach to sample from multi-modal or or hard to sample
 distributions.
\end_layout

\begin_layout Itemize
Like SA we define a sequence of distributions.
\end_layout

\begin_layout Itemize
The difference is we sample from all distributions simultaneously.
\end_layout

\begin_layout Itemize
The chains interact by swapping parameters occasionally.
\end_layout

\begin_deeper
\begin_layout Itemize
This allows values from the easy to sample from chain move to the chain
 of interest.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
PT mode hopping
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename /home/andrew/Documents/teaching/CONTRA/figures/module_5/pt_1.png
	lyxscale 10
	width 45text%

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Graphics
	filename /home/andrew/Documents/teaching/CONTRA/figures/module_5/pt_2.png
	lyxscale 10
	width 45text%

\end_inset


\begin_inset Newline newline
\end_inset


\size tiny
Earl et al.
 Physical chemistry chemical physics 2005
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Approximate Bayesian computation
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
ABC has emerged as an extremely general method for computing posterior approxima
tions.
\end_layout

\begin_layout Itemize
Unlike the other approaches, ABC does require evaluation of the likelihood.
\end_layout

\begin_layout Itemize
This can be extremely useful in population genetics models where it is easy
 to simulate from the model, but hard to evaluate the data probability.
\end_layout

\begin_layout Itemize
The basic idea of ABC is to propose parameters from the prior, and then
 simulate data with these parameters.
 If the simulated data is close enough to the observed data then we add
 the parameters to the posterior approximation.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
ABC
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename /home/andrew/Documents/teaching/CONTRA/figures/module_5/abc.png
	lyxscale 10
	height 85theight%

\end_inset


\begin_inset Newline newline
\end_inset


\size tiny
Sunnåker et al.
 PLOS Computational Biology 2013
\end_layout

\end_deeper
\end_body
\end_document
